#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å·®åˆ†æ›´æ–°å¯¾å¿œæ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æœ€æ–°æ—¥ã‹ã‚‰å®Ÿè¡Œæ—¥ã¾ã§ã®å·®åˆ†ã®ã¿ã‚’åŠ¹ç‡çš„ã«å–å¾—
"""

import yfinance as yf
import pandas as pd
import time
import logging
from datetime import datetime, timedelta
import mysql.connector
import os
import sys
from typing import List, Dict, Optional, Tuple
from business_day_utils import BusinessDayCalculator

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('incremental_stock_collection.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class IncrementalStockCollector:
    def __init__(self, delay_seconds=1.0, batch_size=50, max_retries=3, test_limit=None):
        """
        å·®åˆ†æ›´æ–°å¯¾å¿œæ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¹
        
        Args:
            delay_seconds: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
            test_limit: ãƒ†ã‚¹ãƒˆç”¨ã®å‡¦ç†éŠ˜æŸ„æ•°åˆ¶é™
        """
        self.delay_seconds = delay_seconds
        self.batch_size = batch_size
        self.max_retries = max_retries
        self.base_start_date = "2025-01-06"  # 2025å¹´å¤§ç™ºä¼š
        self.test_limit = test_limit
        self.business_day_calc = BusinessDayCalculator()
        
    def get_db_connection(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’å–å¾—"""
        return mysql.connector.connect(
            host=os.getenv('DB_HOST', 'localhost'),
            user=os.getenv('DB_USER', 'stock_user'),
            password=os.getenv('DB_PASSWORD', 'stock_password'),
            database=os.getenv('DB_NAME', 'chart'),
            charset='utf8mb4',
            collation='utf8mb4_unicode_ci'
        )

    def get_all_stocks_from_master(self) -> List[Dict]:
        """ãƒã‚¹ã‚¿ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å…¨éŠ˜æŸ„ã‚’å–å¾—"""
        conn = self.get_db_connection()
        cursor = conn.cursor(dictionary=True)
        
        try:
            query = """
            SELECT code, symbol, name, sector 
            FROM stock_master 
            ORDER BY code
            """
            cursor.execute(query)
            stocks = cursor.fetchall()
            logger.info(f"ãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰{len(stocks)}éŠ˜æŸ„ã‚’å–å¾—ã—ã¾ã—ãŸ")
            return stocks
            
        finally:
            cursor.close()
            conn.close()

    def get_stock_date_range(self, symbol: str) -> Tuple[Optional[str], Optional[str]]:
        """
        éŠ˜æŸ„ã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ç¯„å›²ã‚’å–å¾—
        
        Returns:
            (start_date, end_date) ã¾ãŸã¯ (None, None)
        """
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            query = """
            SELECT 
                MIN(date) as start_date,
                MAX(date) as end_date,
                COUNT(*) as record_count
            FROM stocks 
            WHERE symbol = %s
            """
            cursor.execute(query, (symbol,))
            result = cursor.fetchone()
            
            if result and result[2] > 0:  # record_count > 0
                return result[0].strftime('%Y-%m-%d'), result[1].strftime('%Y-%m-%d')
            else:
                return None, None
            
        finally:
            cursor.close()
            conn.close()

    def calculate_fetch_period(self, symbol: str, target_end_date: str) -> Tuple[str, str, str]:
        """
        å–å¾—ã™ã¹ãæœŸé–“ã‚’è¨ˆç®—
        
        Args:
            symbol: éŠ˜æŸ„ã‚·ãƒ³ãƒœãƒ«
            target_end_date: ç›®æ¨™çµ‚äº†æ—¥ï¼ˆé€šå¸¸ã¯å®Ÿè¡Œæ—¥ï¼‰
            
        Returns:
            (fetch_start_date, fetch_end_date, status)
            status: 'new', 'update', 'skip'
        """
        existing_start, existing_end = self.get_stock_date_range(symbol)
        
        if existing_start is None:
            # æ–°è¦å–å¾—: 2025/01/06ã‹ã‚‰å®Ÿè¡Œæ—¥ã¾ã§
            return self.base_start_date, target_end_date, 'new'
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š
        existing_end_date = datetime.strptime(existing_end, '%Y-%m-%d')
        target_end_date_obj = datetime.strptime(target_end_date, '%Y-%m-%d')
        
        if existing_end_date >= target_end_date_obj:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒæœ€æ–°: ã‚¹ã‚­ãƒƒãƒ—
            return existing_end, target_end_date, 'skip'
        
        # å·®åˆ†æ›´æ–°: æ—¢å­˜ã®æœ€æ–°æ—¥ã®ç¿Œæ—¥ã‹ã‚‰å®Ÿè¡Œæ—¥ã¾ã§
        next_day = existing_end_date + timedelta(days=1)
        fetch_start = next_day.strftime('%Y-%m-%d')
        
        return fetch_start, target_end_date, 'update'

    def fetch_stock_data(self, symbol: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """
        å˜ä¸€éŠ˜æŸ„ã®ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«å–å¾—
        """
        for attempt in range(self.max_retries):
            try:
                logger.debug(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­: {symbol} ({start_date} ï½ {end_date}) "
                           f"(è©¦è¡Œ {attempt + 1}/{self.max_retries})")
                
                ticker = yf.Ticker(symbol)
                data = ticker.history(start=start_date, end=end_date)
                
                if data.empty:
                    logger.debug(f"ãƒ‡ãƒ¼ã‚¿ãªã—: {symbol} ({start_date} ï½ {end_date})")
                    return None
                
                logger.debug(f"å–å¾—æˆåŠŸ: {symbol} ({len(data)}ä»¶)")
                return data
                
            except Exception as e:
                logger.warning(f"å–å¾—ã‚¨ãƒ©ãƒ¼ {symbol} (è©¦è¡Œ {attempt + 1}): {e}")
                if attempt < self.max_retries - 1:
                    wait_time = (attempt + 1) * 2
                    logger.debug(f"ãƒªãƒˆãƒ©ã‚¤ã¾ã§{wait_time}ç§’å¾…æ©Ÿ...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ã¾ã—ãŸ: {symbol}")
        
        return None

    def save_stock_data(self, symbol: str, data: pd.DataFrame) -> bool:
        """æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        if data.empty:
            return True
            
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            records = []
            for date, row in data.iterrows():
                records.append({
                    'symbol': symbol,
                    'date': date.strftime('%Y-%m-%d'),
                    'open': float(row['Open']),
                    'high': float(row['High']),
                    'low': float(row['Low']),
                    'close': float(row['Close']),
                    'volume': int(row['Volume'])
                })
            
            query = """
            INSERT INTO stocks (symbol, date, open, high, low, close, volume)
            VALUES (%(symbol)s, %(date)s, %(open)s, %(high)s, %(low)s, %(close)s, %(volume)s)
            ON DUPLICATE KEY UPDATE
            open = VALUES(open),
            high = VALUES(high),
            low = VALUES(low),
            close = VALUES(close),
            volume = VALUES(volume)
            """
            
            cursor.executemany(query, records)
            conn.commit()
            logger.debug(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜å®Œäº†: {symbol} ({len(records)}ä»¶)")
            return True
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼ {symbol}: {e}")
            return False
            
        finally:
            cursor.close()
            conn.close()

    def get_data_statistics(self):
        """ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã‚’å–å¾—"""
        conn = self.get_db_connection()
        cursor = conn.cursor(dictionary=True)
        
        try:
            # å…¨ä½“çµ±è¨ˆ
            cursor.execute("""
                SELECT 
                    COUNT(DISTINCT symbol) as unique_stocks,
                    COUNT(*) as total_records,
                    MIN(date) as earliest_date,
                    MAX(date) as latest_date
                FROM stocks
            """)
            overall_stats = cursor.fetchone()
            
            # éŠ˜æŸ„åˆ¥æœ€æ–°æ—¥çµ±è¨ˆ
            cursor.execute("""
                SELECT 
                    date as latest_date,
                    COUNT(*) as stock_count
                FROM (
                    SELECT symbol, MAX(date) as date
                    FROM stocks
                    GROUP BY symbol
                ) as latest_by_stock
                GROUP BY date
                ORDER BY date DESC
                LIMIT 5
            """)
            latest_dates = cursor.fetchall()
            
            return overall_stats, latest_dates
            
        finally:
            cursor.close()
            conn.close()

    def collect_incremental_data(self, target_end_date: Optional[str] = None):
        """
        å·®åˆ†ãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹ç‡çš„ã«å–å¾—
        
        Args:
            target_end_date: ç›®æ¨™çµ‚äº†æ—¥ï¼ˆNoneã®å ´åˆã¯å®Ÿè¡Œæ—¥ï¼‰
        """
        if target_end_date is None:
            # å®Ÿè¡Œæ—¥ã®ç›´è¿‘å–¶æ¥­æ—¥ã‚’å–å¾—
            business_date = self.business_day_calc.get_latest_business_day()
            target_end_date = self.business_day_calc.format_date(business_date)
        else:
            # æŒ‡å®šæ—¥ã®ç›´è¿‘å–¶æ¥­æ—¥ã‚’å–å¾—
            business_date = self.business_day_calc.get_latest_business_day(target_end_date)
            target_end_date = self.business_day_calc.format_date(business_date)
            
        logger.info("=== å·®åˆ†æ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ ===")
        logger.info(f"åŸºæº–é–‹å§‹æ—¥: {self.base_start_date}")
        logger.info(f"ç›®æ¨™çµ‚äº†æ—¥: {target_end_date} (å–¶æ¥­æ—¥èª¿æ•´æ¸ˆã¿)")
        logger.info(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”: {self.delay_seconds}ç§’")
        
        # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
        overall_stats, latest_dates = self.get_data_statistics()
        logger.info("=== ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ ===")
        logger.info(f"ç™»éŒ²éŠ˜æŸ„æ•°: {overall_stats.get('unique_stocks', 0)}")
        logger.info(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {overall_stats.get('total_records', 0):,}")
        if overall_stats.get('earliest_date'):
            logger.info(f"æœ€å¤ãƒ‡ãƒ¼ã‚¿: {overall_stats['earliest_date']}")
            logger.info(f"æœ€æ–°ãƒ‡ãƒ¼ã‚¿: {overall_stats['latest_date']}")
        
        logger.info("=== æœ€æ–°æ—¥åˆ¥éŠ˜æŸ„æ•° ===")
        for stat in latest_dates:
            logger.info(f"{stat['latest_date']}: {stat['stock_count']}éŠ˜æŸ„")
        
        # å…¨éŠ˜æŸ„å–å¾—
        all_stocks = self.get_all_stocks_from_master()
        if not all_stocks:
            logger.error("éŠ˜æŸ„ãƒã‚¹ã‚¿ãƒ¼ãŒç©ºã§ã™")
            return
            
        # ãƒ†ã‚¹ãƒˆåˆ¶é™ã®é©ç”¨
        if self.test_limit is not None:
            all_stocks = all_stocks[:self.test_limit]
            logger.info(f"ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: æœ€åˆã®{len(all_stocks)}éŠ˜æŸ„ã®ã¿å‡¦ç†ã—ã¾ã™")
        
        # å‡¦ç†è¨ˆç”»ã®åˆ†æ
        new_stocks = 0
        update_stocks = 0
        skip_stocks = 0
        total_expected_days = 0
        
        logger.info("=== å‡¦ç†è¨ˆç”»ã®åˆ†æä¸­... ===")
        
        for i, stock in enumerate(all_stocks):
            if i % 100 == 0:
                logger.info(f"åˆ†æé€²æ—: {i+1}/{len(all_stocks)}")
                
            symbol = stock['symbol']
            fetch_start, fetch_end, status = self.calculate_fetch_period(symbol, target_end_date)
            
            if status == 'new':
                new_stocks += 1
                # 2025/01/06ã‹ã‚‰å®Ÿè¡Œæ—¥ã¾ã§ã®å–¶æ¥­æ—¥æ•°ã‚’æ¦‚ç®—
                days = (datetime.strptime(target_end_date, '%Y-%m-%d') - 
                       datetime.strptime(self.base_start_date, '%Y-%m-%d')).days
                total_expected_days += days
            elif status == 'update':
                update_stocks += 1
                # å·®åˆ†æ—¥æ•°ã‚’æ¦‚ç®—
                days = (datetime.strptime(fetch_end, '%Y-%m-%d') - 
                       datetime.strptime(fetch_start, '%Y-%m-%d')).days
                total_expected_days += days
            else:
                skip_stocks += 1
        
        logger.info("=== å‡¦ç†è¨ˆç”» ===")
        logger.info(f"æ–°è¦å–å¾—: {new_stocks}éŠ˜æŸ„")
        logger.info(f"å·®åˆ†æ›´æ–°: {update_stocks}éŠ˜æŸ„")
        logger.info(f"ã‚¹ã‚­ãƒƒãƒ—: {skip_stocks}éŠ˜æŸ„")
        logger.info(f"å‡¦ç†å¯¾è±¡: {new_stocks + update_stocks}éŠ˜æŸ„")
        logger.info(f"æ¨å®šãƒ‡ãƒ¼ã‚¿é‡: {total_expected_days:,}ãƒ¬ã‚³ãƒ¼ãƒ‰")
        
        estimated_time = (new_stocks + update_stocks) * self.delay_seconds
        logger.info(f"æ¨å®šå‡¦ç†æ™‚é–“: {estimated_time/60:.1f}åˆ†")
        
        # å®Ÿéš›ã®å‡¦ç†é–‹å§‹
        logger.info("=== å®Ÿéš›ã®å‡¦ç†é–‹å§‹ ===")
        
        processed_stocks = 0
        successful_new = 0
        successful_update = 0
        failed_stocks = 0
        total_new_records = 0
        
        for batch_start in range(0, len(all_stocks), self.batch_size):
            batch_end = min(batch_start + self.batch_size, len(all_stocks))
            batch_stocks = all_stocks[batch_start:batch_end]
            
            logger.info(f"=== ãƒãƒƒãƒ {batch_start//self.batch_size + 1} "
                       f"({batch_start + 1}-{batch_end}/{len(all_stocks)}) ===")
            
            for stock in batch_stocks:
                symbol = stock['symbol']
                name = stock['name']
                processed_stocks += 1
                
                try:
                    # å–å¾—æœŸé–“è¨ˆç®—
                    fetch_start, fetch_end, status = self.calculate_fetch_period(symbol, target_end_date)
                    
                    if status == 'skip':
                        logger.debug(f"ã‚¹ã‚­ãƒƒãƒ— ({processed_stocks}/{len(all_stocks)}): {name} ({symbol})")
                        continue
                    
                    # ãƒ‡ãƒ¼ã‚¿å–å¾—
                    logger.info(f"å‡¦ç†ä¸­ ({processed_stocks}/{len(all_stocks)}): "
                               f"{name} ({symbol}) [{status}] {fetch_start}ï½{fetch_end}")
                    
                    data = self.fetch_stock_data(symbol, fetch_start, fetch_end)
                    
                    if data is not None and not data.empty:
                        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
                        if self.save_stock_data(symbol, data):
                            if status == 'new':
                                successful_new += 1
                                logger.info(f"âœ… æ–°è¦: {name} ({symbol}) {len(data)}ä»¶")
                            else:
                                successful_update += 1
                                logger.info(f"ğŸ”„ æ›´æ–°: {name} ({symbol}) {len(data)}ä»¶")
                            total_new_records += len(data)
                        else:
                            failed_stocks += 1
                            logger.error(f"âŒ ä¿å­˜å¤±æ•—: {name} ({symbol})")
                    else:
                        if status == 'new':
                            logger.warning(f"âš ï¸  ãƒ‡ãƒ¼ã‚¿ãªã—: {name} ({symbol})")
                        else:
                            logger.info(f"â„¹ï¸  å·®åˆ†ãªã—: {name} ({symbol})")
                
                except Exception as e:
                    failed_stocks += 1
                    logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {name} ({symbol}) - {e}")
                
                # é€²æ—è¡¨ç¤º
                if processed_stocks % 50 == 0:
                    progress = (processed_stocks / len(all_stocks)) * 100
                    logger.info(f"é€²æ—: {progress:.1f}% "
                               f"(æ–°è¦:{successful_new}, æ›´æ–°:{successful_update}, "
                               f"å¤±æ•—:{failed_stocks}, æ–°ãƒ¬ã‚³ãƒ¼ãƒ‰:{total_new_records:,})")
                
                # å¾…æ©Ÿ
                if processed_stocks < len(all_stocks):
                    time.sleep(self.delay_seconds)
        
        # æœ€çµ‚çµæœ
        logger.info("=== å·®åˆ†å–å¾—å®Œäº† ===")
        logger.info(f"å‡¦ç†æ¸ˆã¿éŠ˜æŸ„: {processed_stocks}")
        logger.info(f"æ–°è¦å–å¾—æˆåŠŸ: {successful_new}")
        logger.info(f"å·®åˆ†æ›´æ–°æˆåŠŸ: {successful_update}")
        logger.info(f"å¤±æ•—: {failed_stocks}")
        logger.info(f"æ–°è¦è¿½åŠ ãƒ¬ã‚³ãƒ¼ãƒ‰: {total_new_records:,}ä»¶")
        
        # æœ€çµ‚çµ±è¨ˆ
        final_stats, _ = self.get_data_statistics()
        logger.info("=== æ›´æ–°å¾Œãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ ===")
        logger.info(f"ç·éŠ˜æŸ„æ•°: {final_stats.get('unique_stocks', 0)}")
        logger.info(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {final_stats.get('total_records', 0):,}")
        logger.info(f"æœ€æ–°ãƒ‡ãƒ¼ã‚¿: {final_stats.get('latest_date', 'N/A')}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # è¨­å®š
    delay_seconds = 1.0
    batch_size = 50
    target_date = None  # Noneã§å®Ÿè¡Œæ—¥
    test_limit = None
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°
    if len(sys.argv) > 1:
        if "--fast" in sys.argv:
            delay_seconds = 0.5
            logger.warning("âš ï¸  é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰: 0.5ç§’é–“éš”")
        elif "--slow" in sys.argv:
            delay_seconds = 2.0
            logger.info("ğŸŒ å®‰å…¨ãƒ¢ãƒ¼ãƒ‰: 2ç§’é–“éš”")
        
        if "--date" in sys.argv:
            try:
                date_idx = sys.argv.index("--date")
                target_date = sys.argv[date_idx + 1]
                logger.info(f"ğŸ“… æŒ‡å®šæ—¥ã¾ã§å–å¾—: {target_date}")
            except (ValueError, IndexError):
                logger.error("--date ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã¯æ—¥ä»˜ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ (YYYY-MM-DD)")
                return
        
        if "--batch" in sys.argv:
            try:
                batch_idx = sys.argv.index("--batch")
                batch_size = int(sys.argv[batch_idx + 1])
                logger.info(f"ğŸ“¦ ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
            except (ValueError, IndexError):
                logger.error("--batch ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã¯æ•°å€¤ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
                return
        
        if "--test-limit" in sys.argv:
            try:
                limit_idx = sys.argv.index("--test-limit")
                test_limit = int(sys.argv[limit_idx + 1])
                logger.info(f"ğŸ§ª ãƒ†ã‚¹ãƒˆåˆ¶é™: {test_limit}éŠ˜æŸ„")
            except (ValueError, IndexError):
                logger.error("--test-limit ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã¯æ•°å€¤ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
                return
    
    # ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼å®Ÿè¡Œ
    collector = IncrementalStockCollector(
        delay_seconds=delay_seconds,
        batch_size=batch_size,
        test_limit=test_limit
    )
    
    collector.collect_incremental_data(target_date)

if __name__ == "__main__":
    main()